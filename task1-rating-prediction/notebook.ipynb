{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a12f98e",
   "metadata": {},
   "source": [
    "# Task 1: Yelp Rating Prediction via Prompting (Gemini)\n",
    "\n",
    "Prompt-based 1–5 star prediction using Gemini (no OpenAI). Includes multiple prompt strategies and evaluation.\n",
    "\n",
    "**Sections**\n",
    "- Setup (Gemini client)\n",
    "- Data sampling\n",
    "- Prompt strategies (>=3)\n",
    "- Inference & JSON validation\n",
    "- Evaluation & comparison\n",
    "- Save results\n",
    "- Notes / To-Do\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b43be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: imports and config\n",
    "import os, json, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Env\n",
    "GEMINI_API_KEY = os.getenv('GOOGLE_API_KEY') or os.getenv('GEMINI_API_KEY')\n",
    "MODEL_NAME = os.getenv('MODEL_NAME', 'gemini-1.5-flash')\n",
    "\n",
    "if GEMINI_API_KEY:\n",
    "    genai.configure(api_key=GEMINI_API_KEY)\n",
    "else:\n",
    "    print('⚠️ GOOGLE_API_KEY/GEMINI_API_KEY not set; inference will be skipped.')\n",
    "\n",
    "data_path = Path('data')\n",
    "results_path = Path('results')\n",
    "for p in (data_path, results_path):\n",
    "    p.mkdir(exist_ok=True)\n",
    "\n",
    "print('Python ready. Data dir:', data_path.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6236a010",
   "metadata": {},
   "source": [
    "## Prompt Strategy 1 (Baseline)\n",
    "Simple instruction-only prompt that asks for star rating and brief reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3490a8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_prompt = \"\"\"\n",
    "You are a strict JSON generator. Given a Yelp review, output a JSON object with:\n",
    "- predicted_stars: integer 1-5\n",
    "- explanation: brief reason\n",
    "\n",
    "Return **only** JSON.\n",
    "Review: {review_text}\n",
    "\"\"\"\n",
    "print(baseline_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5794974d",
   "metadata": {},
   "source": [
    "## Prompt Strategy 2 (Chain-of-Thought Lite)\n",
    "Adds lightweight reasoning before final JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865d13ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_prompt = \"\"\"\n",
    "You are an assistant that reasons step-by-step then outputs JSON only.\n",
    "Steps:\n",
    "1) Briefly reason about sentiment, service, food, ambiance, price.\n",
    "2) Decide 1-5 stars.\n",
    "3) Output JSON only: {\"predicted_stars\": <int>, \"explanation\": \"...\"}\n",
    "\n",
    "Review: {review_text}\n",
    "\"\"\"\n",
    "print(cot_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faa8a59",
   "metadata": {},
   "source": [
    "## Prompt Strategy 3 (Self-Check)\n",
    "Adds validation to correct JSON/rating if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec04516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selfcheck_prompt = \"\"\"\n",
    "You are a rating model with a self-check step.\n",
    "- Draft a rating 1-5 with a short reason.\n",
    "- If draft rating not in 1-5 or JSON invalid, fix it.\n",
    "- Return final JSON only: {\"predicted_stars\": <int>, \"explanation\": \"...\"}\n",
    "\n",
    "Review: {review_text}\n",
    "\"\"\"\n",
    "print(selfcheck_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8101ec6e",
   "metadata": {},
   "source": [
    "## Data Sampling\n",
    "Loads `data/yelp_sample.csv` (200 rows sampled). Replace file if you want a different sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6477f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = data_path / 'yelp_sample.csv'\n",
    "if not dataset_path.exists():\n",
    "    raise FileNotFoundError('data/yelp_sample.csv missing. Please provide a sample with columns text, stars.')\n",
    "\n",
    "df = pd.read_csv(dataset_path)\n",
    "sample_size = min(200, len(df))\n",
    "df_sample = df.sample(sample_size, random_state=42) if len(df) > sample_size else df.copy()\n",
    "print('Loaded rows:', len(df), 'Using sample:', len(df_sample))\n",
    "df_sample.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca7d166",
   "metadata": {},
   "source": [
    "## Inference Helper (Gemini)\n",
    "Calls Gemini chat API and parses JSON; skips if no API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e92d063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "system_prompt = 'You output only JSON with predicted_stars and explanation.'\n",
    "\n",
    "def run_prompt(prompt_template: str, review_text: str):\n",
    "    if not GEMINI_API_KEY:\n",
    "        return {'predicted_stars': None, 'explanation': 'no_api_key'}\n",
    "    prompt = prompt_template.format(review_text=review_text)\n",
    "    model = genai.GenerativeModel(model_name=MODEL_NAME, system_instruction=system_prompt)\n",
    "    resp = model.generate_content(prompt)\n",
    "    content = resp.text\n",
    "    try:\n",
    "        return json.loads(content)\n",
    "    except Exception:\n",
    "        start = content.find('{')\n",
    "        end = content.rfind('}')\n",
    "        if start != -1 and end != -1 and end > start:\n",
    "            try:\n",
    "                return json.loads(content[start:end+1])\n",
    "            except Exception:\n",
    "                return {'predicted_stars': None, 'explanation': 'parse_error', 'raw': content}\n",
    "        return {'predicted_stars': None, 'explanation': 'parse_error', 'raw': content}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aeb21e2",
   "metadata": {},
   "source": [
    "## Evaluation Loop\n",
    "Runs each strategy; skips inference if no API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc46fa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = {\n",
    "    'baseline': baseline_prompt,\n",
    "    'cot': cot_prompt,\n",
    "    'selfcheck': selfcheck_prompt,\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, tmpl in strategies.items():\n",
    "    for _, row in df_sample.iterrows():\n",
    "        review_text = row['text']\n",
    "        gt = int(row['stars'])\n",
    "        pred_obj = run_prompt(tmpl, review_text)\n",
    "        pred = pred_obj.get('predicted_stars')\n",
    "        valid_json = pred is not None and isinstance(pred, (int, float)) and 1 <= pred <= 5\n",
    "        results.append({\n",
    "            'strategy': name,\n",
    "            'review': review_text,\n",
    "            'ground_truth': gt,\n",
    "            'pred': pred,\n",
    "            'valid_json': valid_json,\n",
    "            'explanation': pred_obj.get('explanation', '')\n",
    "        })\n",
    "\n",
    "res_df = pd.DataFrame(results)\n",
    "res_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d1f098",
   "metadata": {},
   "source": [
    "## Metrics & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2e7e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(df):\n",
    "    total = len(df)\n",
    "    valid = df['valid_json'].mean()\n",
    "    acc = (df['pred'] == df['ground_truth']).mean()\n",
    "    return pd.Series({'samples': total, 'json_valid_rate': valid, 'accuracy': acc})\n",
    "\n",
    "if len(res_df):\n",
    "    summary = res_df.groupby('strategy').apply(summarize)\n",
    "    print(summary)\n",
    "else:\n",
    "    summary = pd.DataFrame()\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14e94f9",
   "metadata": {},
   "source": [
    "## Save Results\n",
    "Exports detailed predictions and summary to `results/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6078523",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path.mkdir(exist_ok=True)\n",
    "res_out = results_path / 'task1_predictions.csv'\n",
    "sum_out = results_path / 'task1_summary.csv'\n",
    "if len(res_df):\n",
    "    res_df.to_csv(res_out, index=False)\n",
    "    if 'summary' in locals() and not summary.empty:\n",
    "        summary.to_csv(sum_out)\n",
    "    print('Saved', res_out, 'and', sum_out)\n",
    "else:\n",
    "    print('No results to save (likely missing API key).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee44e8fb",
   "metadata": {},
   "source": [
    "## Notes / To-Do\n",
    "- Set `GOOGLE_API_KEY` (or `GEMINI_API_KEY`) and optional `MODEL_NAME` (default: gemini-1.5-flash) before running inference.\n",
    "- Replace `data/yelp_sample.csv` if you want a different sample.\n",
    "- Consider adding retry/backoff and temperature sweeps.\n",
    "- Include these outputs in the report.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
