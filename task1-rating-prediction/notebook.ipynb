{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "333e066c",
      "metadata": {},
      "source": [
        "# Task 1: Yelp Rating Prediction via Prompting (OpenRouter)\n",
        "Prompt-driven 1\u20135 star prediction on Yelp reviews with JSON output. Meets PDF requirements: >=3 prompt strategies, accuracy + JSON validity + reliability/consistency, comparison table, and documented prompt iterations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d911b99d",
      "metadata": {},
      "source": [
        "## What this notebook does\n",
        "- Loads a ~200-row Yelp sample (`data/yelp_sample.csv`).\n",
        "- Runs at least 3 prompt strategies against an OpenRouter-hosted model.\n",
        "- Validates strict JSON schema `{predicted_stars, explanation}` and enforces rating 1\u20135.\n",
        "- Computes accuracy, JSON validity, and consistency (repeat-run agreement on a subset).\n",
        "- Saves detailed predictions and summary tables to `results/`.\n",
        "- Optional: ML baseline for reference.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f0190c6",
      "metadata": {},
      "source": [
        "## Environment and dependencies\n",
        "Set `OPENROUTER_API_KEY` in `.env` or the environment. Optional: `MODEL_NAME` (default: meta-llama/Meta-Llama-3-70B-Instruct), `TEMPERATURE`, `MAX_TOKENS`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "1db8a987",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.3\n",
            "[notice] To update, run: C:\\Users\\HP\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install -q python-dotenv requests typing_extensions pydantic scikit-learn pandas numpy tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "77264606",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Imports and config\n",
        "import os, json, random, time\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from dotenv import load_dotenv\n",
        "import requests\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "load_dotenv()\n",
        "OPENROUTER_API_KEY = os.getenv('OPENROUTER_API_KEY') or os.getenv('OR_API_KEY')\n",
        "MODEL_NAME = os.getenv('MODEL_NAME', 'mistralai/mistral-7b-instruct')\n",
        "SYSTEM_PROMPT = 'You return only JSON with keys predicted_stars (int 1-5) and explanation (string). No prose or markdown.'\n",
        "TEMPERATURE = float(os.getenv('TEMPERATURE', '0.0'))\n",
        "MAX_TOKENS = int(os.getenv('MAX_TOKENS', '128'))\n",
        "USE_JSON_FORMAT = os.getenv('USE_JSON_FORMAT', 'true').lower() == 'true'\n",
        "\n",
        "if not OPENROUTER_API_KEY:\n",
        "    print('OpenRouter API key not set; LLM inference will be skipped.')\n",
        "\n",
        "headers = {\n",
        "    'Authorization': f'Bearer {OPENROUTER_API_KEY}' if OPENROUTER_API_KEY else '',\n",
        "    'HTTP-Referer': os.getenv('OPENROUTER_REFERRER', 'https://localhost'),\n",
        "    'X-Title': os.getenv('OPENROUTER_APP', 'fynd-task1'),\n",
        "}\n",
        "\n",
        "BASE_URL = os.getenv('OPENROUTER_BASE_URL', 'https://openrouter.ai/api/v1/chat/completions')\n",
        "\n",
        "data_path = Path('data')\n",
        "results_path = Path('results')\n",
        "data_path.mkdir(exist_ok=True)\n",
        "results_path.mkdir(exist_ok=True)\n",
        "N_SAMPLE = 200\n",
        "CONSISTENCY_ROWS = 30  # subset size for consistency check\n",
        "CONSISTENCY_RUNS = int(os.getenv('CONSISTENCY_RUNS', '0'))   # set 0 to skip by default\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "761cffe5",
      "metadata": {},
      "source": [
        "## Dataset\n",
        "Uses `data/yelp_sample.csv` with columns `text` (review) and `stars` (ground truth). Sample up to 200 rows for quick iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "0303ff51",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 200 rows; using sample of 200.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stars</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>We got here around midnight last Friday... the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>Brought a friend from Louisiana here.  She say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Every friday, my dad and I eat here. We order ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>My husband and I were really, really disappoin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Love this place!  Was in phoenix 3 weeks for w...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   stars                                               text\n",
              "0      4  We got here around midnight last Friday... the...\n",
              "1      5  Brought a friend from Louisiana here.  She say...\n",
              "2      3  Every friday, my dad and I eat here. We order ...\n",
              "3      1  My husband and I were really, really disappoin...\n",
              "4      5  Love this place!  Was in phoenix 3 weeks for w..."
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "dataset_path = data_path / 'yelp_sample.csv'\n",
        "if not dataset_path.exists():\n",
        "    raise FileNotFoundError('data/yelp_sample.csv missing. Provide columns: text, stars.')\n",
        "\n",
        "df = pd.read_csv(dataset_path)\n",
        "sample_size = min(N_SAMPLE, len(df))\n",
        "df_sample = df.sample(sample_size, random_state=RANDOM_SEED) if len(df) > sample_size else df.copy()\n",
        "print(f'Loaded {len(df)} rows; using sample of {len(df_sample)}.')\n",
        "df_sample.head()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "917bd812",
      "metadata": {},
      "source": [
        "\n",
        "## Prompt strategies (three+)\n",
        "1) **Baseline**: Direct ask for JSON rating + brief reason.\n",
        "2) **Chain-of-thought lite**: Brief reasoning steps before JSON.\n",
        "3) **Guarded self-check**: Draft, validate range, fix JSON, then output.\n",
        "4) **Rubric-guided**: Uses an explicit star rubric to anchor the class choice.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2eb48d07",
      "metadata": {},
      "source": [
        "\n",
        "### Why these prompt versions\n",
        "- Baseline: minimal JSON-only ask; sets the target schema.\n",
        "- Chain-of-thought lite: nudges the model to reason about aspects (service/food/etc.) before answering to lift accuracy.\n",
        "- Guarded self-check: adds explicit validation and repair steps to reduce out-of-range ratings and malformed JSON.\n",
        "- Rubric-guided: anchors the class decision to an explicit 1-5 rubric to reduce ambiguity and improve accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "e941c5f7",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "baseline_prompt = \"\"\"\n",
        "You are a strict JSON generator. Given a Yelp review, output a JSON object with:\n",
        "- predicted_stars: integer 1-5\n",
        "- explanation: brief reason\n",
        "Respond with JSON only (no markdown, no prose, no code fences).\n",
        "Review: {review_text}\n",
        "\"\"\"\n",
        "\n",
        "cot_prompt = \"\"\"\n",
        "You reason briefly then output JSON only.\n",
        "Steps:\n",
        "1) Consider sentiment, service, food, ambiance, and price/value.\n",
        "2) Decide 1-5 stars using this rubric: 1=awful, 2=poor, 3=mixed/ok, 4=good, 5=excellent.\n",
        "3) Output JSON only (no text): {{\"predicted_stars\": <int>, \"explanation\": \"...\"}}\n",
        "Review: {review_text}\n",
        "\"\"\"\n",
        "\n",
        "selfcheck_prompt = \"\"\"\n",
        "You draft a rating and self-check it.\n",
        "- Draft rating 1-5 with a short reason.\n",
        "- If rating outside 1-5 or JSON invalid, fix it using this rubric: 1=awful, 2=poor, 3=mixed/ok, 4=good, 5=excellent.\n",
        "- Return final JSON only (no text): {{\"predicted_stars\": <int>, \"explanation\": \"...\"}}\n",
        "Review: {review_text}\n",
        "\"\"\"\n",
        "\n",
        "rubric_prompt = \"\"\"\n",
        "Classify the review strictly using this rubric and respond with JSON only (no text):\n",
        "1 = awful (strongly negative)\n",
        "2 = poor (mostly negative)\n",
        "3 = mixed/ok (balanced or neutral)\n",
        "4 = good (mostly positive)\n",
        "5 = excellent (strongly positive)\n",
        "Output JSON: {{\"predicted_stars\": <int>, \"explanation\": \"brief reason\"}}\n",
        "Review: {review_text}\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f98999f",
      "metadata": {},
      "source": [
        "## Helpers: JSON parsing, validation, and model call\n",
        "- Enforces schema and rating range.\n",
        "- Attempts to repair responses that wrap JSON in prose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "ec3ae458",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import re\n",
        "\n",
        "MAX_REPAIRS = 1\n",
        "\n",
        "\n",
        "def parse_json_str(text: str):\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except Exception:\n",
        "        start = text.find('{')\n",
        "        end = text.rfind('}')\n",
        "        if start != -1 and end != -1 and end > start:\n",
        "            try:\n",
        "                return json.loads(text[start:end+1])\n",
        "            except Exception:\n",
        "                return None\n",
        "    return None\n",
        "\n",
        "def validate_payload(raw):\n",
        "    if not isinstance(raw, dict):\n",
        "        return None, False, 'not a dict'\n",
        "    if 'predicted_stars' not in raw or 'explanation' not in raw:\n",
        "        return None, False, 'missing keys'\n",
        "    try:\n",
        "        rating = int(round(float(raw['predicted_stars'])))\n",
        "    except Exception:\n",
        "        return None, False, 'bad rating type'\n",
        "    if rating < 1 or rating > 5:\n",
        "        return None, False, 'rating out of range'\n",
        "    explanation = str(raw.get('explanation', '')).strip()\n",
        "    payload = {'predicted_stars': rating, 'explanation': explanation}\n",
        "    return payload, True, None\n",
        "\n",
        "def parse_rating_from_text(text: str):\n",
        "    m = re.search(r'[1-5]', text)\n",
        "    if m:\n",
        "        val = int(m.group(0))\n",
        "        if 1 <= val <= 5:\n",
        "            return val\n",
        "    return None\n",
        "\n",
        "def build_body(prompt: str):\n",
        "    body = {\n",
        "        'model': MODEL_NAME,\n",
        "        'messages': [\n",
        "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
        "            {'role': 'user', 'content': prompt},\n",
        "        ],\n",
        "        'temperature': TEMPERATURE,\n",
        "        'max_tokens': MAX_TOKENS,\n",
        "    }\n",
        "    if USE_JSON_FORMAT:\n",
        "        body['response_format'] = {'type': 'json_object'}\n",
        "    return body\n",
        "\n",
        "def call_model(prompt_template: str, review_text: str):\n",
        "    if not OPENROUTER_API_KEY:\n",
        "        raise RuntimeError('missing_openrouter_key')\n",
        "    prompt = prompt_template.format(review_text=review_text)\n",
        "    resp = requests.post(BASE_URL, headers=headers, json=build_body(prompt), timeout=60)\n",
        "    try:\n",
        "        resp.raise_for_status()\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f'http_error: {resp.status_code} {resp.text}')\n",
        "    data = resp.json()\n",
        "    return data['choices'][0]['message']['content']\n",
        "\n",
        "def repair_response(review_text: str, raw: str):\n",
        "    fix_prompt = f\"\"\"\n",
        "You must return valid JSON only (no markdown, no extra text).\n",
        "Keys: predicted_stars (int 1-5), explanation (string).\n",
        "Use the rubric 1=awful, 2=poor, 3=mixed/ok, 4=good, 5=excellent.\n",
        "Review: {review_text}\n",
        "Previous invalid output: {raw}\n",
        "Return ONLY corrected JSON.\n",
        "\"\"\"\n",
        "    try:\n",
        "        resp = requests.post(BASE_URL, headers=headers, json=build_body(fix_prompt), timeout=60)\n",
        "        resp.raise_for_status()\n",
        "        data = resp.json()\n",
        "        return data['choices'][0]['message']['content']\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def run_prompt(prompt_template: str, review_text: str):\n",
        "    if not OPENROUTER_API_KEY:\n",
        "        return {'predicted_stars': None, 'explanation': 'no_api_key'}\n",
        "    attempts = 0\n",
        "    content = None\n",
        "    while attempts <= MAX_REPAIRS:\n",
        "        try:\n",
        "            if attempts == 0:\n",
        "                content = call_model(prompt_template, review_text)\n",
        "            else:\n",
        "                content = repair_response(review_text, content or '')\n",
        "        except Exception as e:\n",
        "            return {'predicted_stars': None, 'explanation': f'call_error: {e}'}\n",
        "        parsed = parse_json_str(content) if content else None\n",
        "        payload, ok, err = validate_payload(parsed) if parsed is not None else (None, False, 'parse_error')\n",
        "        if ok:\n",
        "            return payload\n",
        "        attempts += 1\n",
        "    rating_guess = parse_rating_from_text(content or '')\n",
        "    if rating_guess:\n",
        "        return {'predicted_stars': rating_guess, 'explanation': 'parsed_from_raw', 'raw': content}\n",
        "    return {'predicted_stars': None, 'explanation': 'parse_error', 'raw': content}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b31e24b3",
      "metadata": {},
      "source": [
        "## Strategy registry\n",
        "Tune temperature or other knobs per prompt if desired."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "e14204b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "strategies = {\n",
        "    'baseline': baseline_prompt,\n",
        "    'cot': cot_prompt,\n",
        "    'selfcheck': selfcheck_prompt,\n",
        "    'rubric': rubric_prompt,\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9100efa1",
      "metadata": {},
      "source": [
        "## Inference loop (main evaluation set)\n",
        "Runs each strategy once over the sampled dataset. Skips if no API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "d26d18b9",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \r"
          ]
        }
      ],
      "source": [
        "\n",
        "from typing import Dict, List\n",
        "\n",
        "def run_batch(df_in, tag: str, repeats: int = 1):\n",
        "    rows: List[Dict] = []\n",
        "    for rep in range(repeats):\n",
        "        for strat, prompt in strategies.items():\n",
        "            for idx, row in tqdm(df_in.iterrows(), total=len(df_in), leave=False):\n",
        "                review = row['text']\n",
        "                truth = row['stars']\n",
        "                result = run_prompt(prompt, review)\n",
        "                pred_raw = result.get('predicted_stars')\n",
        "                pred = int(pred_raw) if isinstance(pred_raw, (int, float)) else None\n",
        "                payload, ok, err = validate_payload(result)\n",
        "                rows.append({\n",
        "                    'strategy': strat,\n",
        "                    'tag': tag,\n",
        "                    'repeat': rep,\n",
        "                    'sample_id': int(idx),\n",
        "                    'review': review,\n",
        "                    'ground_truth': truth,\n",
        "                    'pred': payload['predicted_stars'] if ok else None,\n",
        "                    'valid_json': ok,\n",
        "                    'explanation': payload['explanation'] if ok else result.get('explanation', ''),\n",
        "                    'raw': result.get('raw', None),\n",
        "                })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "if not OPENROUTER_API_KEY:\n",
        "    print('Skipping LLM inference; set OPENROUTER_API_KEY to run.')\n",
        "    res_eval = pd.DataFrame()\n",
        "else:\n",
        "    res_eval = run_batch(df_sample, tag='eval', repeats=1)\n",
        "    res_eval.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "325ddaeb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "explanation\n",
            "The reviewer praises the barber shop as the best they've ever been to, highlighting the excellent service and attention to detail.                                                                             2\n",
            "The reviewer had a wonderful time and highly recommends the performance, praising the actors, the music, and the overall experience.                                                                           2\n",
            "The review is short and lacks detail, making it difficult to fully understand the reviewer's experience.                                                                                                       2\n",
            "Mixed experience with good food but poor service.                                                                                                                                                              2\n",
            "The reviewer found the food decent but overpriced, the service good, and the atmosphere pretentious. They would not return and only gave a higher rating due to the service.                                   2\n",
            "The reviewer had a positive experience, praising the quality of the smoothies and the reasonable price.                                                                                                        2\n",
            "Mostly positive with minor complaints.                                                                                                                                                                         2\n",
            "The reviewer praised the pizza and highlighted a specific appetizer, indicating a very positive experience.                                                                                                    1\n",
            "Convenient location and quick rental car access, but limited airline options.                                                                                                                                  1\n",
            "The reviewer had a positive experience with the burger and fries, noting improvements from a previous visit. They also mentioned a preference for the combo pack and had minor suggestions for improvement.    1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "if len(res_eval):                                                                                 \n",
        "    print(res_eval['explanation'].value_counts().head(10))                                        \n",
        "else:\n",
        "    print(\"res_eval empty\") "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07e62ca4",
      "metadata": {},
      "source": [
        "## Consistency measurement\n",
        "Repeat runs on a smaller subset and compute agreement rates per strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "046ff476",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "if OPENROUTER_API_KEY and CONSISTENCY_RUNS > 0:\n",
        "    df_consistency = df_sample.head(min(CONSISTENCY_ROWS, len(df_sample)))\n",
        "    res_consistency = run_batch(df_consistency, tag='consistency', repeats=CONSISTENCY_RUNS)\n",
        "else:\n",
        "    res_consistency = pd.DataFrame()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a633a8f",
      "metadata": {},
      "source": [
        "## Metrics and comparison table\n",
        "- `json_valid_rate`: share of outputs that passed schema/range checks.\n",
        "- `accuracy`: match rate vs ground truth.\n",
        "- `consistency_rate`: fraction of subset samples where all repeats agreed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "945abc30",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_21392\\820298508.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  summary = res_eval.groupby('strategy').apply(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>samples</th>\n",
              "      <th>json_valid_rate</th>\n",
              "      <th>accuracy</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>strategy</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>baseline</th>\n",
              "      <td>200.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.635</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cot</th>\n",
              "      <td>200.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rubric</th>\n",
              "      <td>200.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>selfcheck</th>\n",
              "      <td>200.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.675</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           samples  json_valid_rate  accuracy\n",
              "strategy                                     \n",
              "baseline     200.0              1.0     0.635\n",
              "cot          200.0              1.0     0.685\n",
              "rubric       200.0              1.0     0.700\n",
              "selfcheck    200.0              1.0     0.675"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary = pd.DataFrame()\n",
        "consistency_table = pd.DataFrame()\n",
        "\n",
        "if len(res_eval):\n",
        "    summary = res_eval.groupby('strategy').apply(\n",
        "        lambda g: pd.Series({\n",
        "            'samples': len(g),\n",
        "            'json_valid_rate': g['valid_json'].mean(),\n",
        "            'accuracy': (g['pred'] == g['ground_truth']).mean(),\n",
        "        })\n",
        "    )\n",
        "\n",
        "if len(res_consistency):\n",
        "    grouped = res_consistency.groupby(['strategy', 'sample_id'])['pred']\n",
        "    agreement = grouped.apply(lambda s: s.nunique(dropna=True) == 1)\n",
        "    consistency_table = agreement.groupby('strategy').mean().rename('consistency_rate').to_frame()\n",
        "\n",
        "summary_all = summary.join(consistency_table, how='left')\n",
        "summary_all"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd1973f7",
      "metadata": {},
      "source": [
        "## Save results\n",
        "Writes detailed predictions and summaries to `results/`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "795d03e2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved results\\task1_predictions.csv and results\\task1_summary.csv\n"
          ]
        }
      ],
      "source": [
        "res_combined = pd.concat([res_eval, res_consistency], ignore_index=True)\n",
        "\n",
        "if len(res_combined):\n",
        "    preds_out = results_path / 'task1_predictions.csv'\n",
        "    summary_out = results_path / 'task1_summary.csv'\n",
        "    res_combined.to_csv(preds_out, index=False)\n",
        "    summary_all.to_csv(summary_out)\n",
        "    print('Saved', preds_out, 'and', summary_out)\n",
        "else:\n",
        "    print('No LLM results to save (likely missing API key).')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79d83592",
      "metadata": {},
      "source": [
        "## Optional: ML baseline (fast, no API)\n",
        "Train simple TF-IDF + logistic regression models for reference accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "76742cc9",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model word_tfidf_lr: accuracy 0.3500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model char_tfidf_lr: accuracy 0.3750\n",
            "Best ML model: char_tfidf_lr with accuracy 0.3750\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.00      0.00      0.00         4\n",
            "           2       0.00      0.00      0.00         3\n",
            "           3       0.00      0.00      0.00         7\n",
            "           4       0.38      0.88      0.53        16\n",
            "           5       0.33      0.10      0.15        10\n",
            "\n",
            "    accuracy                           0.38        40\n",
            "   macro avg       0.14      0.20      0.14        40\n",
            "weighted avg       0.23      0.38      0.25        40\n",
            "\n",
            "Saved ML outputs to results\\task1_predictions_ml.csv and results\\task1_summary_ml.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "X = df['text']\n",
        "y = df['stars']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y)\n",
        "\n",
        "pipelines = {\n",
        "    'word_tfidf_lr': make_pipeline(\n",
        "        TfidfVectorizer(max_features=50000, ngram_range=(1, 3), min_df=2),\n",
        "        LogisticRegression(max_iter=500, C=4.0, n_jobs=-1)\n",
        "    ),\n",
        "    'char_tfidf_lr': make_pipeline(\n",
        "        TfidfVectorizer(analyzer='char', ngram_range=(3, 5), min_df=2),\n",
        "        LogisticRegression(max_iter=400, C=2.0, n_jobs=-1)\n",
        "    ),\n",
        "}\n",
        "\n",
        "ml_results = []\n",
        "best_name, best_acc, best_clf, best_preds = None, -1, None, None\n",
        "\n",
        "for name, pipeline in pipelines.items():\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    preds = pipeline.predict(X_test)\n",
        "    acc = accuracy_score(y_test, preds)\n",
        "    ml_results.append({'model': name, 'accuracy': acc})\n",
        "    print(f'Model {name}: accuracy {acc:.4f}')\n",
        "    if acc > best_acc:\n",
        "        best_name, best_acc, best_clf, best_preds = name, acc, pipeline, preds\n",
        "\n",
        "print(f'Best ML model: {best_name} with accuracy {best_acc:.4f}')\n",
        "print(classification_report(y_test, best_preds))\n",
        "\n",
        "ml_pred_df = pd.DataFrame({\n",
        "    'model': best_name,\n",
        "    'review': X_test,\n",
        "    'ground_truth': y_test,\n",
        "    'pred': best_preds,\n",
        "})\n",
        "ml_summary = pd.DataFrame(ml_results)\n",
        "ml_summary['best'] = ml_summary['model'] == best_name\n",
        "\n",
        "ml_pred_out = results_path / 'task1_predictions_ml.csv'\n",
        "ml_summary_out = results_path / 'task1_summary_ml.csv'\n",
        "ml_pred_df.to_csv(ml_pred_out, index=False)\n",
        "ml_summary.to_csv(ml_summary_out, index=False)\n",
        "print('Saved ML outputs to', ml_pred_out, 'and', ml_summary_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccd0a5b5",
      "metadata": {},
      "source": [
        "## Discussion and prompt evolution\n",
        "- Baseline \u2192 CoT \u2192 Self-check \u2192 Rubric: started with JSON-only ask, added lightweight reasoning (CoT), added self-correction for range/format, then anchored decisions to an explicit 1\u20135 rubric.\n",
        "- Latest 200-row OpenRouter run (mistral-7b-instruct): rubric=0.70 acc, CoT=0.685, selfcheck=0.67, baseline=0.635; JSON valid rate 1.0 across strategies.\n",
        "- Takeaway: rubric prompt is strongest; CoT close behind; self-check helps but trails; baseline lags.\n",
        "- Next: if needed, rerun with only rubric/CoT to save calls, and add a consistency pass (`CONSISTENCY_RUNS>0`). Keep `USE_JSON_FORMAT=true`; drop it only if a model rejects JSON mode.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}